[
  {
    "objectID": "posts/data-science/index.html",
    "href": "posts/data-science/index.html",
    "title": "Unpacking the Machine Learning Process: A Practical Step-by-Step Guide",
    "section": "",
    "text": "Machine learning (ML) is a powerful tool for solving complex problems by identifying patterns in data. However, to build a successful machine learning model, it’s essential to follow a structured process that takes raw data and transforms it into actionable insights. In this tutorial, I will walk through the essential steps of the ML process using logistic regression model on a real-world dataset: the Pima Indians Diabetes Dataset (the ‘diabetes dataset’).\nTHe demo is coded in Python and the primary machine learning software for modeling is scikit-learn, which is an open-source library for Python that offers an extensive range of supervised and unsupervised learning algorithms and tools for machine learning tasks. You can find all source code that supports the demo in this GitHub repository and the final research report here.\n\nStep 1: Define the Problem\nBefore diving into code or models, the first step is always to define the problem clearly. Understanding the business problem or the task we are trying to solve is critical, as it determines the approach and the tools we’ll use.\n\nWhat problem are we trying to solve?\nIs it a classification task, regression task, or clustering task?\nWhat kind of data do we have, and what’s the expected output?\n\nIn this guide, our goal is to predict whether a person has diabetes based on clinical features like glucose levels, BMI, and age. This is a binary classification problem, where the output variable is whether or not the patient has diabetes.\n\n\nStep 2: Understand and Validate the Data\nMachine learning models rely on high-quality data to learn and make predictions, so it is crucial to understand the data before applying any machine learning techniques.\n\nSource, Collection, and Scope of Data\nThe initial stage in understanding any dataset is to get a high-level overview of its source, collection process, and scope. This includes addressing the following questions:\n\nWhere is the data coming from?\nHow was the data collected?\nHow many observations are there?\nWhat relevant features are available?\n\nBy understanding these aspects, we can assess the dataset’s relevance and reliability for the machine learning task at hand. For example, Pima Indians Diabetes Dataset is well-aligned with the goal of predicting diabetes, as it comes from a reliable source, the National Library of Medicine, is based on standardized medical examinations, and provides a sufficient sample size with 768 observations \\(\\times\\) 8 features for meaningful model training and evaluation.\n\n\nData Validation\nFollowing the top level summary on the dataset, the next critical step is to ensures the quality, integrity, and consistency of the dataset such as checking for issues like missing values, outliers, duplicates, and adhering to expected data formats, types, and ranges. For instance, are the glucose levels within a reasonable range, or are there any negative values in fields like age or pregnancies?\nTools like the Pandera package can automate this process by allowing us to define validation schemas for the dataset, ensuring that each column meets predefined conditions. It is important to note that data validation focuses on identifying and addressing inherent issues in the dataset, rather than making subjective judgments or imposing interpretations on the data. This means, in the diabetes dataset, observations are removed when they contain implausible values, like nulls or zeros, in features where these values would be unrealistic (e.g., glucose or blood pressure).\n\n\n\nStep 3: Split the Data into Training and Testing Sets\nBefore proceeding any further with data exploration, it’s important to split the data into two subsets: one for training and one for testing.\nThis division is essential for evaluating how well our model generalizes to unseen data. A common split is 70% for training and 30% for testing, where the training set is used to teach the model, and the test set is reserved to measure its true performance upon the model is properly trained. Keeping the test set “locked” and untouched until the final evaluation is critical for ensuring that the model’s performance is not influenced by prior knowledge of this data.\nIt is VERY important to perform this split early so we avoid any potential data leakage and adhere to the golden rule of machine learning: the test data cannot influence training the model in anyway.\n\n\nStep 4: Exploratory Data Analysis (EDA)\nOnce we’ve split the dataset, we can dive deeper into Exploratory Data Analysis (EDA) on the training data. EDA involves investigating the dataset’s structure and characteristics using both visualizations and statistical methods. By looking at key aspects such as missing values, the distribution of features, and visualizing how different features relate to the target variable, we gain a clear understanding of the data, uncover patterns, and identify any potential issues that might affect model performance.\nIn our demo, Figure 1 visualized the distributions of each predictor from the diabetes training set, with the distributions color-coded by class (0: blue, 1: orange). This exploration often helps identify patterns or trends that inform feature engineering later in the process.\n\n\n\n\n\n\nFigure 1: Comparison of the empirical distributions of training data predictors between those non-diabetic and diabetic. Image by Author.\n\n\n\nMoroever, we also examined correlation among the predictors (Figure 2) to identify multicollinearity, which could cause problems when performing logistic regression. No multicollinearity was detected, as the highest correlation between predictors is below the 0.7 threshold.\n\n\n\n\n\n\nFigure 2: Pearson and Spearman correlations across all features. Image by Author.\n\n\n\n\n\nStep 5: Data Preprocessing\nThis phase, known as data preprocessing, involves cleaning and transforming the raw data to make it suitable for machine learning algorithms. It includes handling missing values, transforming features, and performing feature engineering. Proper data preprocessing is crucial as it directly impacts the performance and accuracy of the machine learning model.\n\nHandling Missing Data\nAddressing missing or invalid data is one of the first preprocessing tasks. Typically missing data is handled by:\n\nRemoving rows with missing values if they are few and don’t significantly affect the dataset.\nImputing missing values using a statistical method, such as replacing them with the median, mean, or mode of the feature, or more sophisticated techniques such as predictive imputation.\n\nChoosing the right approach for missing data depends on the nature of the dataset and the amount of missing data present.\n\n\nFeature Transformation\nFeature transformation ensures that different types of data are appropriately scaled or encoded so that machine learning algorithms can interpret them correctly. The methods used vary depending on the type of data: numeric, categorical, and textual.\n1. Numeric Data\nFor numeric data, many machine learning algorithms benefit from scaling the features to a similar range. This helps ensure that one feature does not disproportionately influence the model compared to other features. Common approaches to numeric feature transformation include:\n\nStandardization: Rescales features to have a mean of 0 and a standard deviation of 1, often used for algorithms like logistic regression.\nNormalization: Scales features to a fixed range (e.g., 0 to 1), commonly used in distance-based algorithms like k-nearest neighbors (k-NN) model.\n\n2. Categorical Data (Nominal and Ordinal)\nCategorical data consists of non-numeric values that represent discrete categories or labels, which cannot be directly used by machine learning models. Common methods of transformtion include:\n\nOne-Hot Encoding: This method creates binary (0 or 1) columns for each category, turning a single categorical feature with multiple categories into several binary features. This is often used for nominal data, where the categories do not have a meaningful order (e.g., “female” vs. “male”).\nLabel Encoding: This approach converts each category into a numeric label, which is suitable for ordinal data where there is a meaningful order (e.g., “low”, “medium”, “high”).\n\n3. Text Data\nRaw text data, such as sentences, documents, or reviews, contains valuable information but cannot be directly interpreted by algorithms. Feature transformation techniques are used to convert text into a numerical format for machine learning models to process effectively.\n\nBag-of-Words: Transforms text into numerical features by counting word occurrences in documents, creating a sparse matrix.\nTF-IDF (Term Frequency-Inverse Document Frequency): Weighs word frequencies by their importance across the dataset, down-weighting common words like “the” and “is.”\n\n\n\nFeature Engineering\nFeature engineering refers creating new features from the existing dataset to better represent the underlying patterns in the data. This step is often key to improving the predictive power of the model. This often includes creating interaction terms (e.g. squared or cubic terms), adding polynomial features, and sometimes aggregating multiple features.\nIn our case, the diabetes dataset already contains structured numeric data that only required scaling, but feature engineering such as interaction terms could have been explored to better capture relationships between features.\nImportant Note on Data Integrity\nThe procedures and techniques in this section highlight why it’s crucial to split the data before starting any preprocessing. For example, imagine applying a feature scaling technique like standardization, where we calculate the mean and standard deviation of a feature. If we compute these statistics using both the training and testing data, we would inadvertently introduce information from the test set into the training process, resulting in data leakage by giving the model an unfair advantage and inflating its performance.\nBy splitting the data first, we ensure the integrity of the test set for unbiased evaluation is preserved. Mechanically, we fit the transformation models (such as scaling or encoding) only on the training data, and then apply the same transformations to both the training and testing data, ensuring consistency without contaminating the test set.\n\n\n\nStep 6: Choose a Model\nAt this stage, choose a machine learning model to solve our problem. The choice of model depends on the problem type, the nature of the data, and the available computational resources.\n\nClassification problems: Logistic regression, decision trees, random forests, support vector machines (SVM), k-NN, etc.\nRegression problems: Linear regression, decision trees, random forests, etc.\nClustering problems: K-means, DBSCAN, hierarchical clustering, etc.\n\nFor this tutorial, we chose Logistic Regression, a simple yet powerful model for binary classification tasks. It models the probability of a binary outcome (e.g. diabetic vs. non-diabetic) based on the input features.\n\n\nStep 7: Train the Model\nOnce we’ve selected the model, it’s time to train it on the training data. Training involves feeding the features and corresponding target values to the model so that it can learn the patterns and relationships between them.\n\nCross Validation\nTypically, we use cross-validation technique during training to improve the generalization ability of a machine learning model. It works by splitting the training data into multiple subsets, training the model on some folds, and validating it on the remaining fold. This process is repeated for each fold, and the average performance (along with standard deviation) is used to assess the model. Cross-validation helps mitigate overfitting and provides a more reliable estimate of the model’s performance on unseen data.\n\n\nHyperparameter Tuning\nFor many machine learning models, there are hyperparameters that need to be tuned to optimize performance. Unlike model parameters, which are learned during training, hyperparameters are set before training and control the model’s learning process. Examples of hyperparameters include the learning rate in gradient descent, the number of trees in a random forest, or the regularization strength in logistic regression. Fine-tuning these hyperparameters can significantly impact the model’s ability to generalize to unseen data.\nCommon techniques for hyperparameter tuning include:\n\nGrid Search, where a predefined set of hyperparameters is tested exhaustively\nRandom Search, which samples hyperparameters randomly from a specified range.\nAdvanced techniques such as Bayesian optimization, genetic algorithms (GA), hyperband, etc.\n\nIn the demo, the goal is to optimize the model to make accurate predictions for new data. The model finds the best-fit coefficients for the features to minimize the cost function (e.g., the log-loss function) for logistic regression. We’ve using RandomizedSearchCV to find the optimal regularization strength that minimizes the model’s error rate.\n\n\n\nStep 8: Evaluate the Model\nAfter training the model, it’s essential to evaluate its performance on the test set. Evaluation allows us to measure how well the model generalizes to new, unseen data.\n\nEvaluation Metrics\nEvaluation metrics help quantify this generalization by comparing the model’s predictions against the true outcomes. The choice of evaluation metrics depends on the type of problem and the characteristics of the data.\n\nClassification Metrics: accuracy, precision, recall, f1-score, fbeta-score, ROC curve and AUC, PR curve and AP\nRegression Metrics: mean squared error (MSE), mean absolute error (MAE), mean absolute percentage error (mape), and R-squared (\\(R^2\\))\nClustering Metrics: Clustering is an unsupervised learning task, and its evaluation metrics typically focus on measuring the similarity or compactness of the clusters formed. Evaluation metrics can be categorized into internal metrics, which assess clustering quality based solely on the data itself, and external metrics, which compare the clusters to pre-defined categories or ground truth.\n\nInternal metrics: silhouette score, Davies-Bouldin index, and Dunn index\nExternal metrics: Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Fowlkes-Mallows Index (FMI)\n\n\nGenerally, accuracy for classification and mean squared error (MSE) for regression are set as default evaluation metrics in most machine learning software packages.\nFor the sample analysis, the model reported an accuracy score of 75%. More in depth evaluation of test set predictions include confusion matrix (Figure 3), PR curve (Figure 4), and (Figure 5).\n\n\n\n\n\n\nFigure 3: Confusion matrix of test set prediction accuracy. Image by Author\n\n\n\n\n\n\n\n\n\nFigure 4: Precision recall curve and AP score of test set predictions. Image by Author.\n\n\n\n\n\n\n\n\n\nFigure 5: ROC Curve and AUC score of test set predictions\n\n\n\n\n\n\nStep 9: Refine the Model\nAfter evaluating the model, we might find areas for improvement. This could involve:\n\nFeature Engineering: Trying new features or transformations\nHyperparameter Tuning: Using more advanced techniques to fine-tune the model’s parameters\nTrying Different Models: Experimenting other models if the chosen doesn’t perform well\n\n\n\nStep 10: Deploy the Model\nOnce our model is refined and performing well, the next step is to deploy it in a real-world scenario, which typically involves the following steps:\n\nIntegrate the model into an application or API: This allows the model to make predictions either in real time or in batch mode, depending on the specific needs of the system.\nMonitor the model’s performance: Continuously tracking the model’s effectiveness on new data helps identify any performance drops or changes in data patterns.\nPeriodic model updates: The model may need to be retrained with new data or fine-tuned to maintain its accuracy and relevance as conditions evolve.\n\nOur demo has skipped through step 9 and 10, as the project was designed solely for academic and tutorial purposes. Consequently, the results achieved are not intended to be final or satisfactory for production use.\n\n\nConclusion\nIn this tutorial, we’ve covered the complete machine learning process using the Pima Indians Diabetes Dataset as a practical example. We explored how to define the problem, collect, split, and preprocess the data, train a model, evaluate its performance, and refine it for improved performance. Each of these steps is essential for building a robust machine learning model capable of solving real-world problems.\nMachine learning is an iterative and evolving process that involves continuous experimentation and refinement. As you advance in your data science journey, the principles outlined in this tutorial will guide you in tackling more complex challenges and making data-driven decisions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Unpacking the Machine Learning Process: A Practical Step-by-Step Guide\n\n\nHands-On Demo with Logistic Regression on the Pima Indians Diabetes Dataset\n\n\n\ntutorial\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]