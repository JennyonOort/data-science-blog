{
  "hash": "d10ae0b01f879db3a7d6fbfc746aebf6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unpacking the Machine Learning Process: A Practical Step-by-Step Guide\"\nsubtitle: \"Hands-On Demo with Logistic Regression on the Pima Indians Diabetes Dataset\"\ncategories: [tutorial, code, analysis]\njupyter: python3\nimage: \"image.jpg\"\n---\n\n\n\n\nMachine learning (ML) is a powerful tool for solving complex problems by identifying patterns in data. However, to build a successful machine learning model, it's essential to follow a structured process that takes raw data and transforms it into actionable insights. In this tutorial, I will walk through the essential steps of the ML process using logistic regression model on a real-world dataset: the [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) (the 'diabetes dataset').\n\nTHe demo is coded in [Python](https://www.python.org/) and the primary machine learning software for modeling is [scikit-learn](https://scikit-learn.org/stable/), which is an open-source library for Python that offers an extensive range of supervised and unsupervised learning algorithms and tools for machine learning tasks. You can find all source code that supports the demo in this [GitHub repository](https://github.com/UBC-MDS/diabetes_predictor_py) and the final research report [here](https://ubc-mds.github.io/diabetes_predictor_py/reports/diabetes_analysis.html).\n\n\n### **Step 1: Define the Problem**\n\nBefore diving into code or models, the first step is always to **define the problem** clearly. Understanding the business problem or the task we are trying to solve is critical, as it determines the approach and the tools we'll use.\n\n- **What problem are you trying to solve?**\n- **Is it a classification task, regression task, or clustering task?**\n- **What kind of data do you have, and what’s the expected output?**\n\nIn this guide, our goal is to predict whether a person has diabetes based on clinical features like glucose levels, BMI, and age. This is a binary classification problem, where the output variable is whether or not the patient has diabetes. \n\n### **Step 2: Understand and Validate the Data**\n\nMachine learning models rely on high-quality data to learn and make predictions, so it is crucial to understand the data before applying any machine learning techniques. \n\n#### <u>Source, Collection, and Scope of Data</u>\nThe initial stage in understanding any dataset is to get a high-level overview of its source, collection process, and scope. This includes addressing the following questions:\n\n- **Where is the data coming from?**\n- **How was the data collected?** \n- **How many observations are there?**\n- **What relevant features are available?**\n\nBy understanding these aspects, we can assess the dataset's relevance and reliability for the machine learning task at hand. For example, Pima Indians Diabetes Dataset is well-aligned with the goal of predicting diabetes, as it comes from a reliable source, the National Library of Medicine, is based on standardized medical examinations, and provides a sufficient sample size with 768 observations $\\times$ 8 features for meaningful model training and evaluation.\n\n#### <u>Data Validation</u>\nFollowing the top level summary on the dataset, the next critical step is to ensures the quality, integrity, and consistency of the dataset such as checking for issues like missing values, outliers, duplicates, and adhering to expected data formats, types, and ranges. For instance, are the glucose levels within a reasonable range, or are there any negative values in fields like age or pregnancies? \n\nTools like the [Pandera](https://pandera.readthedocs.io/en/stable/) package can automate this process by allowing us to define validation schemas for the dataset, ensuring that each column meets predefined conditions. It is important to note that data validation focuses on identifying and addressing inherent issues in the dataset, rather than making subjective judgments or imposing interpretations on the data. This means, in the diabetes dataset, observations are removed when they contain implausible values, like nulls or zeros, in features where these values would be unrealistic (e.g., glucose or blood pressure).\n\n### **Step 3: Split the Data into Training and Testing Sets**\n\nBefore proceeding any further with data exploration, it’s important to split the data into two subsets: one for training and one for testing. \n\nThis division is essential for evaluating how well our model generalizes to unseen data. A common split is **70% for training and 30% for testing**, where the training set is used to teach the model, and the test set is reserved to measure its true performance upon the model is properly trained. Keeping the test set \"locked\" and untouched until the final evaluation is critical for ensuring that the model’s performance is not influenced by prior knowledge of this data. \n\nIt is **VERY important** to perform this split early so we avoid any potential **data leakage** and adhere to the **golden rule of machine learning**: the test data cannot influence training the model in anyway. \n\n### **Step 4: Exploratory Data Analysis (EDA)**\n\nOnce we’ve split the dataset, we can dive deeper into Exploratory Data Analysis (EDA) on the **training data**. EDA involves investigating the dataset’s structure and characteristics using both visualizations and statistical methods. By looking at key aspects such as missing values, the distribution of features, and visualizing how different features relate to the target variable, we gain a clear understanding of the data, uncover patterns, and identify any potential issues that might affect model performance.\n\nIn our demo, @fig-feature_histograms visualized the distributions of each predictor from the diabetes training set, with the distributions color-coded by class (0: blue, 1: orange). This exploration often helps identify patterns or trends that inform [feature engineering](#feature-engineering) later in the process. \n\n![Comparison of the empirical distributions of training data predictors between those non-diabetic and diabetic. Image by Author.](figures/feature_histograms.png){#fig-feature_histograms width=80%}\n\nMoroever, we also examined correlation among the predictors (@fig-correlation_heatmap) to identify multicollinearity, which could cause problems when performing logistic regression. No multicollinearity was detected, as the highest correlation between predictors is below the 0.7 threshold.\n\n![Pearson and Spearman correlations across all features. Image by Author.](figures/correlation_heatmap.png){#fig-correlation_heatmap width=80%}\n\n### **Step 5: Data Preprocessing**\n\nThis phase, known as data preprocessing, involves cleaning and transforming the raw data to make it suitable for machine learning algorithms. It includes handling missing values, transforming features, and performing feature engineering. Proper data preprocessing is crucial as it directly impacts the performance and accuracy of the machine learning model.\n\n#### <u>Handling Missing Data</u>\nAddressing missing or invalid data is one of the first preprocessing tasks. Typically missing data is handled by:\n\n- **Removing rows** with missing values if they are few and don't significantly affect the dataset.\n- **Imputing missing values** using a statistical method, such as replacing them with the median, mean, or mode of the feature, or more sophisticated techniques such as predictive imputation. \n\nChoosing the right approach for missing data depends on the nature of the dataset and the amount of missing data present.\n\n#### <u>Feature Transformation</u>\n\nFeature transformation ensures that different types of data are appropriately scaled or encoded so that machine learning algorithms can interpret them correctly. The methods used vary depending on the type of data: numeric, categorical, and textual.\n\n*1. Numeric Data*\n\nFor numeric data, many machine learning algorithms benefit from scaling the features to a similar range. This helps ensure that one feature does not disproportionately influence the model compared to other features. Common approaches to numeric feature transformation include:\n\n- **Standardization:** Rescales features to have a mean of 0 and a standard deviation of 1, often used for algorithms like logistic regression.\n- **Normalization:** Scales features to a fixed range (e.g., 0 to 1), commonly used in distance-based algorithms like k-nearest neighbors (k-NN) model.\n\n*2. Categorical Data (Nominal and Ordinal)*\n\nCategorical data consists of non-numeric values that represent discrete categories or labels, which cannot be directly used by machine learning models. Common methods of transformtion include:\n\n- **One-Hot Encoding:** This method creates binary (0 or 1) columns for each category, turning a single categorical feature with multiple categories into several binary features. This is often used for nominal data, where the categories do not have a meaningful order (e.g., \"female\" vs. \"male\").\n- **Label Encoding:** This approach converts each category into a numeric label, which is suitable for ordinal data where there is a meaningful order (e.g., \"low\", \"medium\", \"high\"). \n\n*3. Text Data*\n\nRaw text data, such as sentences, documents, or reviews, contains valuable information but cannot be directly interpreted by algorithms. Feature transformation techniques are used to convert text into a numerical format for machine learning models to process effectively. \n\n- **Bag-of-Words:** Transforms text into numerical features by counting word occurrences in documents, creating a sparse matrix.\n- **TF-IDF (Term Frequency-Inverse Document Frequency):** Weighs word frequencies by their importance across the dataset, down-weighting common words like \"the\" and \"is.\"\n\n#### <u>Feature Engineering</u>\n\nFeature engineering refers creating new features from the existing dataset to better represent the underlying patterns in the data. This step is often key to improving the predictive power of the model. This often includes creating interaction terms (e.g. squared or cubic terms), adding polynomial features, and sometimes aggregating multiple features. \n\nIn our case, the diabetes dataset already contains structured numeric data that only required scaling, but feature engineering such as interaction terms could have been explored to better capture relationships between features.\n\n***Important Note on Data Integrity***\n\nThe procedures and techniques in this section highlight why it's crucial to [split the data](#step-3-split-the-data-into-training-and-testing-sets) before starting any preprocessing. For example, imagine applying a feature scaling technique like standardization, where we calculate the mean and standard deviation of a feature. If we compute these statistics using both the training and testing data, we would inadvertently introduce information from the test set into the training process, resulting in data leakage by giving the model an unfair advantage and inflating its performance. \n\nBy splitting the data first, we ensure the integrity of the test set for unbiased evaluation is preserved. Mechanically, we fit the transformation models (such as scaling or encoding) only on the training data, and then apply the same transformations to both the training and testing data, ensuring consistency without contaminating the test set.\n\n### **Step 6: Choose a Model**\n\nAt this stage, choose a **machine learning model** to solve your problem. The choice of model depends on the problem type, the nature of the data, and the available computational resources.\n\n- **Classification problems**: [Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [decision trees](https://scikit-learn.org/stable/modules/tree.html), [random forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles), [support vector machines (SVM)](https://scikit-learn.org/stable/modules/svm.html), [k-NN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), etc.\n- **Regression problems**: [Linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), decision trees, random forests, etc.\n- **Clustering problems**: [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html), [hierarchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), etc.\n\nFor this tutorial, we chose **Logistic Regression**, a simple yet powerful model for binary classification tasks. It models the probability of a binary outcome (e.g. diabetic vs. non-diabetic) based on the input features. \n\n### **Step 7: Train the Model**\n\nOnce you’ve selected the model, it’s time to train it on the training data. Training involves feeding the features and corresponding target values to the model so that it can learn the patterns and relationships between them.\n\n#### <u>Cross Validation</u>\n\n#### <u>Hyperparameter Tuning</u>\nFor many machine learning models, there are hyperparameters that need to be tuned to optimize performance. Unlike model parameters, which are learned during training, hyperparameters are set before training and control the model's learning process. Examples of hyperparameters include the learning rate in gradient descent, the number of trees in a random forest, or the regularization strength in logistic regression. Fine-tuning these hyperparameters can significantly impact the model's ability to generalize to unseen data. \n\nCommon techniques for hyperparameter tuning include: \n\n- Grid Search, where a predefined set of hyperparameters is tested exhaustively\n- Random Search, which samples hyperparameters randomly from a specified range. \n- Advanced techniques such as [Bayesian optimization](https://github.com/bayesian-optimization/BayesianOptimization), [genetic algorithms (GA)](https://medium.com/@byanalytixlabs/a-complete-guide-to-genetic-algorithm-advantages-limitations-more-738e87427dbb), [hyperband](https://2020blogfor.github.io/posts/2020/04/hyperband/), etc.\n\nIn the demo, the goal is to optimize the model to make accurate predictions for new data. The model finds the best-fit coefficients (see @tbl-coeff_table) for the features to minimize the cost function (e.g., the log-loss function) for logistic regression. We've using `RandomizedSearchCV` to find the optimal regularization strength that minimizes the model's error rate.\n\n\n\n\n```{r}\n#| label: tbl-coeff_table\n#| tbl-cap: Logistic regression feature importance measured by coefficients.\nlibrary(knitr)\ncoeff_table<- read.csv(\"tables/coeff_table.csv\")\n\nknitr::kable(coeff_table)\n```\n\n\n\n\n## **Step 7: Evaluate the Model**\n\nAfter training the model, it’s essential to evaluate its performance on the test set. Evaluation allows us to measure how well the model generalizes to unseen data.\n\n### **Accuracy and Confusion Matrix**\n\nFor classification tasks, a common evaluation metric is **accuracy**, which measures the proportion of correctly classified instances. However, accuracy alone might not provide a full picture, especially if the dataset is imbalanced (as in our case with more non-diabetic patients than diabetic ones).\n\nTo better understand the model’s performance, we use a **confusion matrix**, which breaks down the predictions into four categories:\n\n- **True Positives (TP)**: Correctly predicted diabetic cases\n- **True Negatives (TN)**: Correctly predicted non-diabetic cases\n- **False Positives (FP)**: Non-diabetic cases incorrectly predicted as diabetic\n- **False Negatives (FN)**: Diabetic cases incorrectly predicted as non-diabetic\n\nThe confusion matrix helps us calculate additional metrics, such as **Precision**, **Recall**, and the **F1-Score**, which give a more nuanced understanding of the model's strengths and weaknesses.\n\n### **Precision, Recall, and F1-Score**\n\n- **Precision** measures how many of the positive predictions are actually correct.\n- **Recall** measures how many actual positive cases were correctly identified.\n- **F1-Score** is the harmonic mean of Precision and Recall and provides a balanced measure of a model’s performance when there’s an imbalance between classes.\n\n### **ROC Curve and AUC**\n\nThe **Receiver Operating Characteristic (ROC) Curve** and **Area Under the Curve (AUC)** are useful for evaluating binary classifiers. The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate. AUC is the area under this curve, where a higher AUC indicates better model performance.\n\n## **Step 8: Refine the Model**\n\nAfter evaluating the model, you might find areas for improvement. This could involve:\n\n- **Feature Engineering**: Trying new features or transformations\n- **Hyperparameter Tuning**: Using more advanced techniques like cross-validation to fine-tune the model’s parameters\n- **Trying Different Models**: If logistic regression doesn’t perform well, you might try models like Random Forests or Gradient Boosting.\n\n## **Step 9: Deploy the Model**\n\nOnce your model is refined and performing well, it’s time to deploy it into a production environment. In a real-world scenario, this could involve:\n\n- Building an application or API that uses the model to make predictions in real time.\n- Monitoring the model’s performance over time to ensure it continues to perform well with new data.\n- Updating the model periodically with new data to ensure it remains accurate and effective.\n\n## **Conclusion**\n\nIn this tutorial, we've walked through the entire machine learning process, , using the Pima Indians Diabetes Dataset as a practical example. We discussed how to define the problem, collect and preprocess data, train a logistic regression model, evaluate its performance, and refine it for better accuracy. Each of these steps is crucial in building an effective machine learning model that can be used for real-world applications.\n\nThe Pima Indians dataset demonstrated that logistic regression is a useful tool for binary classification problems, but it also showed the importance of feature selection, hyperparameter tuning, and model evaluation in achieving the best possible results.\n\nMachine learning is an iterative process that requires experimentation and refinement. As you progress in your data science journey, these foundational steps will serve as a guide to solving increasingly complex problems and making data-driven decisions.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}